{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3ff568d-a3df-45b4-9cda-f20e318785de",
   "metadata": {},
   "source": [
    "# Final Q20: Build Generative Model with Iris Data\n",
    "## Author: Matthew Stickle\n",
    "## Generative model: Gaussian\n",
    "Note: Unlike in the mnist dataset, the probabilities are big enough where we do not have to worry about underflow and thus we do not need to use logpdf. We also do not need to worry about smoothing our covariance matrix since the default covariance matrix is not singular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25223b9c-a4ce-4d9b-a4aa-3b4f5d3f362a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pandas import read_csv\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a3e99e4-e977-46b6-a6b0-e9b2b1c811cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data\n",
    "data = read_csv('./iris.data', header=None)\n",
    "\n",
    "# Prepare data to be split\n",
    "string_labels = data.iloc[:,-1]\n",
    "string_labels = string_labels.tolist()\n",
    "\n",
    "# remap strings to ints for convience\n",
    "mapping = {l:i for i, l in enumerate(np.unique(string_labels))}\n",
    "labels = [mapping[x] for x in string_labels]\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Drop labels from original dataframe, leaving only numerical data to find params with\n",
    "data = data.drop(columns=4)\n",
    "data = data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "212d0b47-6230-4e4b-bdf0-87b11bc68c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train type: <class 'numpy.ndarray'> x_train shape: (135, 4)\n",
      "y_train type: <class 'numpy.ndarray'> y_train: (135,)\n",
      "x_test type: <class 'numpy.ndarray'> x_test shape: (15, 4)\n",
      "y_test type: <class 'numpy.ndarray'> y_test shape: (15,)\n",
      "unique train class: [0 1 2]\n",
      "unique test class: [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=15, random_state=0)\n",
    "# Verify types and dimensions\n",
    "print(f\"x_train type: {type(x_train)} x_train shape: {x_train.shape}\\ny_train type: {type(y_train)} y_train: {y_train.shape}\")\n",
    "print(f\"x_test type: {type(x_test)} x_test shape: {x_test.shape}\\ny_test type: {type(y_test)} y_test shape: {y_test.shape}\")\n",
    "print(f\"unique train class: {np.unique(y_train)}\\nunique test class: {np.unique(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42a450e1-4082-4fa9-adb7-46a4d8c4807e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine class probabilities\n",
    "train_size = x_train.shape[0]\n",
    "iris, counts = np.unique(y_train, return_counts=True)\n",
    "iris_probs = {i: c/train_size for i, c in zip(iris, counts)}\n",
    "assert sum(iris_probs.values()) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ee0da0d-dfbf-4d12-8cc2-819d1ba639a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find mean and cov for each class to build generative gaussian model\n",
    "g = {}\n",
    "for i in iris:\n",
    "    x = x_train[y_train == i, :]\n",
    "    m = np.mean(x, axis = 0)\n",
    "    cov = np.cov(x, rowvar=False)\n",
    "    assert m.shape[0] == cov.shape[0]\n",
    "    g[i] = {\n",
    "        'm':   m,\n",
    "        'cov': cov\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc0195f5-c0a6-4a77-91da-f9e55ea2a4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With mean and cov for each class, time to predict with generative gaussian on test data\n",
    "res = np.full([x_test.shape[0], 3], -np.inf)\n",
    "for i,params in g.items():\n",
    "    pdf = multivariate_normal.pdf(x_test, mean=params['m'], cov=params['cov'])\n",
    "    res[:,i] = (pdf * iris_probs[i])\n",
    "y_pred = np.argmax(res, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2a496c8-8446-4fba-a6d3-53e9b6f79a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error rate of generative gaussian model is 0.0\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(y_test, y_pred)\n",
    "error_rate = 1 - acc\n",
    "print(f\"error rate of generative gaussian model is {error_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fd4878-99f8-4836-93a7-0e4aa29276e5",
   "metadata": {},
   "source": [
    "## Other observations\n",
    "Getting an error rate of 0% is pretty fishy, especially when the testing size is so small. Boosting up the testing size and reducing the training size (ie: 100 test samples versus 15 test samples) did produce a higher error rate. This makes sense since we are reducing the training set and thus have less accurate measures of our class probabilities, mean, and covariance for each class. In the case where I altered the test set to have 100 samples, I found an error rate of 5%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
